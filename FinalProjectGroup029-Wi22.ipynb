{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# COGS 108 - Final Project (change this to your project's title)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Permissions\n",
    "\n",
    "Place an `X` in the appropriate bracket below to specify if you would like your group's project to be made available to the public. (Note that student names will be included (but PIDs will be scraped from any groups who include their PIDs).\n",
    "\n",
    "* [  ] YES - make available\n",
    "* [  ] NO - keep private"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Fill in your overview here*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Names\n",
    "\n",
    "- Stephen Kim\n",
    "- Clara Yi\n",
    "- Ethan Lee\n",
    "- Ernest Lin\n",
    "- Wesley Nguyen"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the macroscopic socioeconomic features of a state, specifically median income, percentage of population without health insurance, and prevalence of blue collar workers, have a correlation to COVID infection and mortality rates in 2020-2021?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='background'></a>\n",
    "\n",
    "## Background & Prior Work"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Introduction\n",
    "When a society faces unusual challenges, it often leads to major cultural shifts and realizations. COVID-19, which has impacted the global society in unpredictable and significant ways, stands as an opportunity for data scientists to gain insight into the nuances of healthcare, labor, and economics. By analyzing information from the CDC's database on COVID-19 related deaths, information from the United States Census Bureau, as well as data from the US Bureau of Labor Statistics, our team hopes to shed light on whether or not a state's overall socioeconomic breakdown influenced their COVID-19 mortality rate in 2020. Our macroscopic approach to this data science problem is motivated by the availability of consistent and state-specific data. We additionally propose a predictive model in the form of a function which uses the analyzed trends to make a prediction of covid mortality rate based on three hypothetical values: a specifed median income, % of population without healthcare, and % of labor force in blue collar jobs. \n",
    "\n",
    "### Prior Work\n",
    "An health policy article by Adhikari, S. et. al [1] discussed the early impact of COVID-19 based on a city's income level and race/ethnicity data. Their paper focused on ten major cities and discovered a positive correlation between lower income, more diverse areas and an increase in COVID-19 death and infection rates. According to an American Medical Association review of Adhikari's publication, there is \"no biological or genetic basis for why these inequities would exist\". While this article suggests an important relationship between income, race, and COVID-19 impact, our research team wishes to better understand this relationship on a state-level scale. This macroscopic approach has also been deemed meaningful by larger organizations such as the NIH, as expressed in a 2020 publication from the Journal of General Internal Medicine [2]. The researchers in this study used the Gini index as their measurement of income inequality. They acknowledge that income levels may be representative of a state's healthcare resources and number of essential occupations, but we believe that by directly analyzing health insurance and labor statistics in our research will paint a clearer picture of what previous scientists have already suggested.\n",
    "\n",
    "References (include links):\n",
    "- 1) https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2768723?resultClick=1\n",
    "- 2) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7313247/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hypothesis\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on our prior research, we hypothesize that there will be a negative correlation between median income and COVID morality rate, positive correlation between the percent of the population without health insurance and COVID morality rate, and a positive correlation between the rate of\"blue collar\" workers among the labor force and COVID mortality rate. By combining our three socioeconomic factors into a summarizing coefficient, we hope to create a predicitive model that reflects this hypothesis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset(s)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset 1\n",
    "\n",
    "- Dataset Name: United States COVID-19 Cases and Deaths by State over Time\n",
    "- Link to the dataset: https://data.cdc.gov/Case-Surveillance/United-States-COVID-19-Cases-and-Deaths-by-State-o/9mfq-cb36/data\n",
    "- Number of observations: 44,280 rows, 15 columns, 664,200 observations total\n",
    "\n",
    "This dataset contains the United States (and underlying US territories) data for its COVID rates over time. Such rates include total cases, new cases, total deaths, new deaths, and other metrics that give an overall view of the statistics of COVID for each state. There are submission dates for each row, so that is how we are going to link the rates to specific periods of time\n",
    "\n",
    "### Dataset 2\n",
    "\n",
    "- Dataset Name: Employees on nonfarm payrolls by state and selected industry sector, seasonally adjusted\n",
    "- Link to the dataset: https://www.bls.gov/news.release/laus.t03.htm\n",
    "- Number of observations: 50 rows, 9 columns, 450 observations total\n",
    "\n",
    "Dataset from the US Bureau of Labor Statistics, counting the total number of employees in thousands in the labor force in each state as well as in each of eight industries (construction, manufacturing, trade/transportation/utilities, finance, services, education/health, leisure/hospitality, government).\n",
    "\n",
    "### Dataset 3\n",
    "\n",
    "- Dataset Name: Median Household Income and Percentage of Americans without Health Insurance in 2020\n",
    "- Link to the dataset: https://docs.google.com/spreadsheets/d/174jFoW8KsXGJmpNUx8cbh6j4l6rhQhpOUKIPnkzk3lM/edit#gid=0\n",
    "- Number of observations: 50 rows, 2 columns, 100 observations total\n",
    "\n",
    "This dataset contains the United States' for the median household income and percentage of Americans without Health Insurance in 2020. This data was taken from two different sources, [United States Census Bureau Website](https://www.census.gov/quickfacts/fact/map/CA/HEA775220) and [Federal Reserve Economic Data](https://fred.stlouisfed.org/release/tables?rid=249&eid=259515&od=2020-01-01#), and all of this data was manually imported into a Google Sheet that was converted to a CSV file. \n",
    "\n",
    "### Merging Data\n",
    "Since we are using 3 different primary datasets, we will identify each state with a unique code (California would be CA, Missouri would be MO, etc.). Ultimately, we will merge the datasets during our analysis, with several rows of data for each state."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# these installations are not directly related to our EDA, but the necessary packages throughout all steps of our project. There are EDA-specific installations later.\n",
    "# pip initial installation\n",
    "!pip3 install pandas\n",
    "!pip install pandas\n",
    "!pip3 install matplotlib\n",
    "!pip3 install seaborn\n",
    "!pip3 install openpyxl\n",
    "!pip3 install sklearn\n",
    "!pip3 install patsy \n",
    "!pip3 install statsmodels\n",
    "!pip3 install openpyxl\n",
    "\n",
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "import patsy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset 1 (COVID)\n",
    "- With the imported data, we removed unncessary states. We only want the 50 states not including territories or DC\n",
    "- We then removed the columns that we didn't need for analysis. We did this by selecting the columns that we needed\n",
    "- We also wanted the dates to appear in a sortable/searchable way, so we made the dates arranged in yyyy-mm-dd format\n",
    "- The data was then saved as a csv file.\n",
    "\n",
    "**Note**: Since the data is arranged by date, we created a function ```read_covid_data``` that will return the 50 states with their respective data for just that specified date"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "## Dataset 1 (COVID) Code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cleaning State Data\n",
    "def clean_covid_data():\n",
    "    # Date Closure\n",
    "    def apply_date(date: str) -> str:\n",
    "        split_date = date.split(\"/\")\n",
    "        return \"/\".join([split_date[2], split_date[0], split_date[1]])\n",
    "        \n",
    "    \n",
    "    # Read the data (already in tabular form)\n",
    "    covid_data_url = r\"./Raw Data/United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\"\n",
    "    covid_data = pd.read_csv(covid_data_url)\n",
    "    \n",
    "    # States we will not be looking at (These aren't part of the 50 states)\n",
    "    remove_states = [\"RMI\", \"FSM\", \"GU\", \"MP\", \"PW\", \"NYC\", \"PR\", \"AS\", \"VI\", \"DC\"]\n",
    "    covid_data = covid_data[~covid_data[\"state\"].isin(remove_states)]\n",
    "    \n",
    "    # Remove columns we don't need\n",
    "    covid_data = covid_data[[\"submission_date\", \"state\", \"tot_cases\", \"tot_death\"]]\n",
    "    \n",
    "    # Change Date format to allow for easier sorting\n",
    "    covid_data[\"submission_date\"] = covid_data[\"submission_date\"].apply(apply_date)\n",
    "    \n",
    "    # Sort Date\n",
    "    covid_data.sort_values(\"submission_date\", inplace=True, ascending=False)\n",
    "    covid_data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Save Data\n",
    "    clean_covid_data_url = r\"Cleaned Data/state_covid_data.csv\"\n",
    "    covid_data.to_csv(clean_covid_data_url, index=False)\n",
    "    \n",
    "clean_covid_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_covid_data(month: int, day: int, year: int):\n",
    "    #retrieve cleaned csv\n",
    "    covid_data_url = r\"Cleaned Data/state_covid_data.csv\"\n",
    "    covid_data = pd.read_csv(covid_data_url)\n",
    "    \n",
    "    #reformat date parameter to match data values in csv, then get all data from specific date\n",
    "    date_filter = formatDate(month, day, year)\n",
    "    covid_data = covid_data[covid_data[\"submission_date\"] == date_filter]\n",
    "    covid_data.sort_values(\"state\", inplace=True)\n",
    "    covid_data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return covid_data\n",
    "\n",
    "def formatPreZero(num: int) -> str:\n",
    "\n",
    "    #adds '0' char to any integer less than 10 to match formatting\n",
    "    if num >= 10:\n",
    "        return str(num)\n",
    "    \n",
    "    return \"0\" + str(num)\n",
    "    \n",
    "#reformat date parameter to match data values in csv    \n",
    "def formatDate(month: int,  day: int, year: int) -> str:\n",
    "    return f\"{year}/{formatPreZero(month)}/{formatPreZero(day)}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset 2 (Labor)\n",
    "- The raw data file for Dataset 2 is an excel file. The format of the data was not organized in a way that complements dataframes, so we had a lot of unnecessary texts in the excel\n",
    "read as data entries as well. \n",
    "- Our first step was to identify the columns and rows that we want, which are the 50 states. We removed unnecessary states (including U.S. territories) and removed all the extra\n",
    "non-state entries that were read as rows.\n",
    "- Another problem is that some names in the State column had some footnote numbers that were unintentionally read from the excel sheet. We solved this by removing all occurences of numbers and parentheses from the State column.\n",
    "- Since other datasets use state codes and the original data uses state names, we had to transform state names in the States column to their corresponding state codes. We did this by defining a function ```to_state_code``` that uses a dictionary to map each state name to their state code.\n",
    "- We had to reorganize the structure of the dataframe, as the original file had the data stacked on top of each other so each state had 3 rows in the Dataframe. We did so by separating the \n",
    "raw dataframe into three different dataframes, and then combining them into a single dataframe so we only have 50 rows.\n",
    "- We then removed unnessary columns, such as data from other time periods (Our focus was December of 2020). We also combined the columns of job sectors into two groups relevant to our analysis: Blue collar (construction, mining, trade, leisure) and White collar (Financial, professional, education, government) jobs.\n",
    "- Our final step for Dataset 3 is to export the cleaned dataset as a csv and save it to the \"Cleaned Data\" folder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset 2 (Labor) Code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Copied from https://gist.github.com/rogerallen/1583593\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "\n",
    "def to_state_code(state_name):\n",
    "    return us_state_to_abbrev[state_name]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clean_labor_data():\n",
    "    #Read excel file, renames first column to States and take out null rows\n",
    "    raw_labor_data = pd.read_excel(\"./Raw Data/labor_dataset_raw.xlsx\", header = 4, engine = \"openpyxl\")\n",
    "    raw_labor_data.rename(columns={\"Unnamed: 0\": \"State\"}, inplace=True)\n",
    "    raw_labor_data = raw_labor_data.dropna()\n",
    "\n",
    "    #Take out data from 2021 and only keep 2020\n",
    "    raw_labor_data = raw_labor_data[[\"State\", \"Dec.\\n2020\", \"Dec.\\n2020.1\", \"Dec.\\n2020.2\"]]\n",
    "    \n",
    "    non_states = [\"Virgin Islands\", \"District of Columbia\", \"Puerto Rico\"]\n",
    "\n",
    "    #Removes all non official states from dataset\n",
    "    for region in non_states:\n",
    "        raw_labor_data = raw_labor_data[raw_labor_data[\"State\"].str.contains(region)==False]\n",
    "\n",
    "    #Reset index to start at 0\n",
    "    raw_labor_data = raw_labor_data.reset_index(drop = True)\n",
    "\n",
    "    #eliminated extra characters in state names\n",
    "    raw_labor_data[\"State\"] = raw_labor_data[\"State\"].str.replace('\\d+', '', regex=True)\n",
    "    raw_labor_data[\"State\"] = raw_labor_data[\"State\"].str.replace('(', '', regex=True)\n",
    "    raw_labor_data[\"State\"] = raw_labor_data[\"State\"].str.replace(')', '', regex=True)\n",
    "\n",
    "    #Convert state names into codes (First two letters of each state name)\n",
    "    raw_labor_data[\"State\"] = raw_labor_data[\"State\"].apply(lambda state_name: us_state_to_abbrev[state_name])\n",
    "\n",
    "    #Original raw data has different columns stacked on top of each row, so we need to reorder the dataset.\n",
    "    #Block 1 contains total, constructing and mining data\n",
    "    block1 = raw_labor_data[:50]\n",
    "    block1.columns = [\"State\", \"Total\", \"Constructing\", \"Mining\"]\n",
    "\n",
    "    #Block 2 contains Trade, Financial and Professional\n",
    "    block2 = raw_labor_data[50:100]\n",
    "    block2.columns = [\"State\", \"Trade\", \"Financial\", \"Professional\"]\n",
    "\n",
    "    #Block 3 contains Education, Leisure and Government\n",
    "    block3 = raw_labor_data[100:]\n",
    "    block3.columns = [\"State\", \"Education\", \"Leisure\", \"Gov\"]\n",
    "\n",
    "    #merge all blocks into one dataframe\n",
    "    labor_data = block1.merge(block2, on=\"State\")\n",
    "    labor_data = labor_data.merge(block3, on=\"State\")\n",
    "\n",
    "    #We only need data on white collar and blue collar, so we can combine each job sector to their respective group.\n",
    "    labor_data[\"Blue_col\"] = labor_data[\"Constructing\"] + labor_data[\"Mining\"] + labor_data[\"Trade\"] + labor_data[\"Leisure\"]\n",
    "    labor_data[\"White_col\"] = labor_data[\"Financial\"] + labor_data[\"Professional\"] + labor_data[\"Education\"] + labor_data[\"Gov\"]\n",
    "\n",
    "    #Get rid of all other columns except State, White_col, Blue_col and Total\n",
    "    labor_data.drop(columns = [\"Constructing\", \"Mining\", \"Trade\", \"Financial\", \"Professional\", \"Education\", \"Leisure\", \"Gov\"], inplace=True)\n",
    "    #export as csv\n",
    "    labor_data.columns = ['state','tot_jobs','blue_col','white_col']\n",
    "    labor_data.to_csv('./Cleaned Data/state_labor_data.csv')\n",
    "\n",
    "def get_labor_data():\n",
    "    labor_data_url = './Cleaned Data/state_labor_data.csv'\n",
    "    labor_data = pd.read_csv(labor_data_url)\n",
    "    return labor_data\n",
    "\n",
    "clean_labor_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Dataset 3 (Income/Insurance)\n",
    "- For Dataset 3, we have two primary steps in cleaning the data. The first step was manually inputting the data from the data sources to a CSV file via Google Sheets. This manual step was necessary due to the fact that the original data source did not have an option to directly extract/download the raw data. Since there were only 50 observations, we decided manual input was the best option. \n",
    "\n",
    "- Our second step for Dataset 3 was to import the data into this notebook. We uploaded the CSV file into our \"Raw Data\" folder, and then used read_csv to bring it into a dataframe, which is a usable format for our future analysis. After making sure there were no issues, we then saved it to the \"Cleaned Data\" folder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset 3 (Income/Insurance) Code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def standardize_income(string):\n",
    "    string = string.strip()\n",
    "\n",
    "    string = string.replace(',', '')\n",
    "    return float(string)\n",
    "\n",
    "# Cleaning socioeconomic data\n",
    "def clean_socioeconomic_data():\n",
    "    socioeconomic_data_url = r'./Raw Data/socioeconomic_data.csv'\n",
    "    soci_data = pd.read_csv(socioeconomic_data_url)\n",
    "\n",
    "    #simplify columns and replace Median Household income values with floats\n",
    "    soci_data.columns = ['state', '%_no_insurance', 'median_income']\n",
    "    soci_data['median_income'] = soci_data['median_income'].apply(standardize_income)\n",
    "\n",
    "    soci_data['with_insurance'] = 100 - soci_data['%_no_insurance'] \n",
    "\n",
    "    # Saving to CSV\n",
    "    clean_socioeconomic_data_url = r\"./Cleaned Data/clean_socioeconomic_data.csv\"\n",
    "    soci_data.to_csv(clean_socioeconomic_data_url, index=False)\n",
    "    \n",
    "def get_socioeconomic_data():\n",
    "    clean_socioeconomic_data_url = r\"./Cleaned Data/clean_socioeconomic_data.csv\"\n",
    "    soci_data = pd.read_csv(clean_socioeconomic_data_url)\n",
    "    return soci_data\n",
    "\n",
    "clean_socioeconomic_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset 4 (US population)\n",
    "- The raw data is in csv format with population census data from the US government. For our purposes, we only want the state name and the population in 2020. Although it is 2022, there is not yet a fully released dataset on 2021 population data aside from estimates, so it is fine that we only have 2020 populations and we will be conducting our analysis for 2020.\n",
    "\n",
    "- Since there are also US territories included and DC, we remove those unwanted rows. We also want to store states as their state code rather than their full name. There are also problems with having the population stored as a string, so we do some formatting to make the population be stored as an integer. Finally, we apply some renaming and then our data cleaning is complete.\n",
    "\n",
    "https://data.ers.usda.gov/reports.aspx?ID=17827"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset 4 (US population) Code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clean_population_data():\n",
    "    def format_population(pop: str) -> int:\n",
    "        pop = pop.replace(\",\", \"\")\n",
    "        return int(pop)\n",
    "    # get population data\n",
    "    population_data_url = r\"Raw Data/PopulationReport.csv\"\n",
    "    pop_data = pd.read_csv(population_data_url)\n",
    "    \n",
    "    # keep only state name and total population 2020\n",
    "    pop_data = pop_data[[\"Name\", \"Pop. 2020\"]]\n",
    "    \n",
    "    # Remove unwanted Rows\n",
    "    remove_rows = [\"United States\", \"District of Columbia\", \"Puerto Rico\", \"Source: U.S. Census Bureau, 1990, 2000, 2010, 2020 Censuses of Population\\n\\n\\n\"]\n",
    "    pop_data = pop_data[~pop_data[\"Name\"].isin(remove_rows)]\n",
    "    pop_data.dropna(inplace=True)\n",
    "    \n",
    "    # Change column name\n",
    "    pop_data.columns = [\"state\", \"total_population\"]\n",
    "    \n",
    "    # Value formatting\n",
    "    pop_data[\"total_population\"] = pop_data[\"total_population\"].apply(format_population)\n",
    "    pop_data[\"state\"] = pop_data[\"state\"].apply(to_state_code)\n",
    "    \n",
    "    # save to cleaned data\n",
    "    cleaned_population_data_url = \"Cleaned Data/population.csv\"\n",
    "    pop_data.to_csv(cleaned_population_data_url, index=False)\n",
    "    \n",
    "def read_population_data():\n",
    "    cleaned_population_data_url = \"Cleaned Data/population.csv\"\n",
    "    pop_data = pd.read_csv(cleaned_population_data_url)\n",
    "    return pop_data\n",
    "\n",
    "clean_population_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Analysis & Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Include cells that describe the steps in your data analysis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ethics & Privacy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Fill in your ethics & privacy discussion here*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion & Discussion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Fill in your discussion information here*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Team Contributions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Specify who in your group worked on which parts of the project.*"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}