{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Stephen Kim\n",
    "- Clara Yi\n",
    "- Ethan Lee\n",
    "- Ernest Lin\n",
    "- Wesley Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the macroscopic socioeconomic features of a state, specifically median income, percentage of population without health insurance, and prevalence of blue collar workers, have a correlation to COVID mortality rate in 2020-2021?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Dataset 1\n",
    "\n",
    "- Dataset Name: United States COVID-19 Cases and Deaths by State over Time\n",
    "- Link to the dataset: https://data.cdc.gov/Case-Surveillance/United-States-COVID-19-Cases-and-Deaths-by-State-o/9mfq-cb36/data\n",
    "- Number of observations: 44,280 rows, 15 columns, 664,200 observations total\n",
    "\n",
    "This dataset contains the United States (and underlying US territories) data for its COVID rates over time. Such rates include total cases, new cases, total deaths, new deaths, and other metrics that give an overall view of the statistics of COVID for each state. There are submission dates for each row, so that is how we are going to link the rates to specific periods of time\n",
    "\n",
    "### Dataset 2\n",
    "\n",
    "- Dataset Name: Employees on nonfarm payrolls by state and selected industry sector, seasonally adjusted\n",
    "- Link to the dataset: https://www.bls.gov/news.release/laus.t03.htm\n",
    "- Number of observations: 50 rows, 9 columns, 450 observations total\n",
    "\n",
    "Dataset from the US Bureau of Labor Statistics, counting the total number of employees in thousands in the labor force in each state as well as in each of eight industries (construction, manufacturing, trade/transportation/utilities, finance, services, education/health, leisure/hospitality, government).\n",
    "\n",
    "### Dataset 3\n",
    "\n",
    "- Dataset Name: Median Household Income and Percentage of Americans without Health Insurance in 2020\n",
    "- Link to the dataset: https://docs.google.com/spreadsheets/d/174jFoW8KsXGJmpNUx8cbh6j4l6rhQhpOUKIPnkzk3lM/edit#gid=0\n",
    "- Number of observations: 50 rows, 2 columns, 100 observations total\n",
    "\n",
    "This dataset contains the United States' for the median household income and percentage of Americans without Health Insurance in 2020. This data was taken from two different sources, [United States Census Bureau Website](https://www.census.gov/quickfacts/fact/map/CA/HEA775220) and [Federal Reserve Economic Data](https://fred.stlouisfed.org/release/tables?rid=249&eid=259515&od=2020-01-01#), and all of this data was manually imported into a Google Sheet that was converted to a CSV file. \n",
    "\n",
    "### Merging Data\n",
    "Since we are using 3 different primary datasets, we will identify each state with a unique code (California would be CA, Missouri would be MO, etc.). Ultimately, we will merge the datasets during our analysis, with several rows of data for each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python\\python39\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\python\\python39\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\python\\python39\\lib\\site-packages (from pandas) (1.21.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\python\\python39\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the 'C:\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python\\python39\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\python\\python39\\lib\\site-packages (from pandas) (1.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\python\\python39\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\python\\python39\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the 'C:\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning (Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your data cleaning steps here.\n",
    "## Dataset 1 (COVID)\n",
    "- With the imported data, we removed unncessary states. We only want the 50 states not including territories or DC\n",
    "- We then removed the columns that we didn't need for analysis. We did this by selecting the columns that we needed\n",
    "- We also wanted the dates to appear in a sortable/searchable way, so we made the dates arranged in yyyy-mm-dd format\n",
    "- The data was then saved as a csv file.\n",
    "\n",
    "**Note**: Since the data is arranged by date, we created a function ```read_covid_data``` that will return the 50 states with their respective data for just that specified date\n",
    "\n",
    "## Dataset 2 (Labor)\n",
    "- The raw data file for Dataset 2 is an excel file. The format of the data was not organized in a way that complements dataframes, so we had a lot of unnecessary texts in the excel\n",
    "read as data entries as well. \n",
    "- Our first step was to identify the columns and rows that we want, which are the 50 states. We removed unnecessary states (including U.S. territories) and removed all the extra\n",
    "non-state entries that were read as rows.\n",
    "- Another problem is that some names in the State column had some footnote numbers that were unintentionally read from the excel sheet. We solved this by removing all occurences of numbers and parentheses from the State column.\n",
    "- Since other datasets use state codes and the original data uses state names, we had to transform state names in the States column to their corresponding state codes. We did this by defining a function ```to_state_code``` that uses a dictionary to map each state name to their state code.\n",
    "- We had to reorganize the structure of the dataframe, as the original file had the data stacked on top of each other so each state had 3 rows in the Dataframe. We did so by separating the \n",
    "raw dataframe into three different dataframes, and then combining them into a single dataframe so we only have 50 rows.\n",
    "- We then removed unnessary columns, such as data from other time periods (Our focus was December of 2020). We also combined the columns of job sectors into two groups relevant to our analysis: Blue collar (construction, mining, trade, leisure) and White collar (Financial, professional, education, government) jobs.\n",
    "- Our final step for Dataset 3 is to export the cleaned dataset as a csv and save it to the \"Cleaned Data\" folder.\n",
    "\n",
    "## Dataset 3 (Income/Insurance)\n",
    "- For Dataset 3, we have two primary steps in cleaning the data. The first step was manually inputting the data from the data sources to a CSV file via Google Sheets. This manual step was necessary due to the fact that the original data source did not have an option to directly extract/download the raw data. Since there were only 50 observations, we decided manual input was the best option. \n",
    "\n",
    "- Our second step for Dataset 3 was to import the data into this notebook. We uploaded the CSV file into our \"Raw Data\" folder, and then used read_csv to bring it into a dataframe, which is a usable format for our future analysis. After making sure there were no issues, we then saved it to the \"Cleaned Data\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1 Code (COVID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\Data\\\\State Data\\\\United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23784/3143022382.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mcovid_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_covid_data_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mclean_covid_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23784/3143022382.py\u001b[0m in \u001b[0;36mclean_covid_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Read the data (already in tabular form)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcovid_data_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\".\\Data\\State Data\\United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mcovid_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovid_data_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# States we will not be looking at (These aren't part of the 50 states)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\Data\\\\State Data\\\\United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv'"
     ]
    }
   ],
   "source": [
    "# Cleaning State Data\n",
    "def clean_covid_data():\n",
    "    # Date Closure\n",
    "    def apply_date(date: str) -> str:\n",
    "        split_date = date.split(\"/\")\n",
    "        return \"/\".join([split_date[2], split_date[0], split_date[1]])\n",
    "        \n",
    "    \n",
    "    # Read the data (already in tabular form)\n",
    "    covid_data_url = r\".\\Data\\State Data\\United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\"\n",
    "    covid_data = pd.read_csv(covid_data_url)\n",
    "    \n",
    "    # States we will not be looking at (These aren't part of the 50 states)\n",
    "    remove_states = [\"RMI\", \"FSM\", \"GU\", \"MP\", \"PW\", \"NYC\", \"PR\", \"AS\", \"VI\", \"DC\"]\n",
    "    covid_data = covid_data[~covid_data[\"state\"].isin(remove_states)]\n",
    "    \n",
    "    # Remove columns we don't need\n",
    "    covid_data = covid_data[[\"submission_date\", \"state\", \"tot_cases\", \"tot_death\"]]\n",
    "    \n",
    "    # Change Date format to allow for easier sorting\n",
    "    covid_data[\"submission_date\"] = covid_data[\"submission_date\"].apply(apply_date)\n",
    "    \n",
    "    # Sort Date\n",
    "    covid_data.sort_values(\"submission_date\", inplace=True, ascending=False)\n",
    "    covid_data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Save Data\n",
    "    clean_covid_data_url = r\".\\Cleaned Data\\state_covid_data.csv\"\n",
    "    covid_data.to_csv(clean_covid_data_url, index=False)\n",
    "    \n",
    "clean_covid_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_date</th>\n",
       "      <th>state</th>\n",
       "      <th>tot_cases</th>\n",
       "      <th>tot_death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>AK</td>\n",
       "      <td>58212</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>AL</td>\n",
       "      <td>507479</td>\n",
       "      <td>10798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>AR</td>\n",
       "      <td>327060</td>\n",
       "      <td>5481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>AZ</td>\n",
       "      <td>834006</td>\n",
       "      <td>16553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>CA</td>\n",
       "      <td>3528795</td>\n",
       "      <td>55330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>CO</td>\n",
       "      <td>452758</td>\n",
       "      <td>6040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>CT</td>\n",
       "      <td>293102</td>\n",
       "      <td>7788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>DE</td>\n",
       "      <td>91768</td>\n",
       "      <td>1511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>FL</td>\n",
       "      <td>1943062</td>\n",
       "      <td>33574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>GA</td>\n",
       "      <td>1034763</td>\n",
       "      <td>18262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>HI</td>\n",
       "      <td>28312</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>IA</td>\n",
       "      <td>343073</td>\n",
       "      <td>5642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>ID</td>\n",
       "      <td>175657</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>IL</td>\n",
       "      <td>1210113</td>\n",
       "      <td>23216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>IN</td>\n",
       "      <td>674087</td>\n",
       "      <td>12834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>KS</td>\n",
       "      <td>298218</td>\n",
       "      <td>4835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>KY</td>\n",
       "      <td>420608</td>\n",
       "      <td>7164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>LA</td>\n",
       "      <td>437565</td>\n",
       "      <td>9903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>MA</td>\n",
       "      <td>603464</td>\n",
       "      <td>16739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>MD</td>\n",
       "      <td>394716</td>\n",
       "      <td>8587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>ME</td>\n",
       "      <td>47388</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>MI</td>\n",
       "      <td>718912</td>\n",
       "      <td>16887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>MN</td>\n",
       "      <td>498218</td>\n",
       "      <td>6815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>MO</td>\n",
       "      <td>559832</td>\n",
       "      <td>8687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>MS</td>\n",
       "      <td>302329</td>\n",
       "      <td>7129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>MT</td>\n",
       "      <td>101933</td>\n",
       "      <td>1393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>NC</td>\n",
       "      <td>882865</td>\n",
       "      <td>12562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>ND</td>\n",
       "      <td>101150</td>\n",
       "      <td>1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>NE</td>\n",
       "      <td>206034</td>\n",
       "      <td>2128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>NH</td>\n",
       "      <td>78813</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>NJ</td>\n",
       "      <td>840685</td>\n",
       "      <td>23925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>NM</td>\n",
       "      <td>188488</td>\n",
       "      <td>3741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>NV</td>\n",
       "      <td>299471</td>\n",
       "      <td>5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>NY</td>\n",
       "      <td>966982</td>\n",
       "      <td>18431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>OH</td>\n",
       "      <td>990340</td>\n",
       "      <td>18036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>OK</td>\n",
       "      <td>438347</td>\n",
       "      <td>7053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>OR</td>\n",
       "      <td>159788</td>\n",
       "      <td>2324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>PA</td>\n",
       "      <td>973678</td>\n",
       "      <td>24587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>RI</td>\n",
       "      <td>130864</td>\n",
       "      <td>2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>SC</td>\n",
       "      <td>538682</td>\n",
       "      <td>8811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>SD</td>\n",
       "      <td>114649</td>\n",
       "      <td>1912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>TN</td>\n",
       "      <td>785983</td>\n",
       "      <td>11627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>TX</td>\n",
       "      <td>2721126</td>\n",
       "      <td>47807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>UT</td>\n",
       "      <td>378600</td>\n",
       "      <td>2027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>VA</td>\n",
       "      <td>597141</td>\n",
       "      <td>10104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>VT</td>\n",
       "      <td>15761</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>WA</td>\n",
       "      <td>350342</td>\n",
       "      <td>5135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>WI</td>\n",
       "      <td>625690</td>\n",
       "      <td>7177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>WV</td>\n",
       "      <td>135678</td>\n",
       "      <td>2531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2021/03/15</td>\n",
       "      <td>WY</td>\n",
       "      <td>55327</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   submission_date state  tot_cases  tot_death\n",
       "0       2021/03/15    AK      58212        331\n",
       "1       2021/03/15    AL     507479      10798\n",
       "2       2021/03/15    AR     327060       5481\n",
       "3       2021/03/15    AZ     834006      16553\n",
       "4       2021/03/15    CA    3528795      55330\n",
       "5       2021/03/15    CO     452758       6040\n",
       "6       2021/03/15    CT     293102       7788\n",
       "7       2021/03/15    DE      91768       1511\n",
       "8       2021/03/15    FL    1943062      33574\n",
       "9       2021/03/15    GA    1034763      18262\n",
       "10      2021/03/15    HI      28312        448\n",
       "11      2021/03/15    IA     343073       5642\n",
       "12      2021/03/15    ID     175657       1916\n",
       "13      2021/03/15    IL    1210113      23216\n",
       "14      2021/03/15    IN     674087      12834\n",
       "15      2021/03/15    KS     298218       4835\n",
       "16      2021/03/15    KY     420608       7164\n",
       "17      2021/03/15    LA     437565       9903\n",
       "18      2021/03/15    MA     603464      16739\n",
       "19      2021/03/15    MD     394716       8587\n",
       "20      2021/03/15    ME      47388        725\n",
       "21      2021/03/15    MI     718912      16887\n",
       "22      2021/03/15    MN     498218       6815\n",
       "23      2021/03/15    MO     559832       8687\n",
       "24      2021/03/15    MS     302329       7129\n",
       "25      2021/03/15    MT     101933       1393\n",
       "26      2021/03/15    NC     882865      12562\n",
       "27      2021/03/15    ND     101150       1458\n",
       "28      2021/03/15    NE     206034       2128\n",
       "29      2021/03/15    NH      78813       1199\n",
       "30      2021/03/15    NJ     840685      23925\n",
       "31      2021/03/15    NM     188488       3741\n",
       "32      2021/03/15    NV     299471       5121\n",
       "33      2021/03/15    NY     966982      18431\n",
       "34      2021/03/15    OH     990340      18036\n",
       "35      2021/03/15    OK     438347       7053\n",
       "36      2021/03/15    OR     159788       2324\n",
       "37      2021/03/15    PA     973678      24587\n",
       "38      2021/03/15    RI     130864       2588\n",
       "39      2021/03/15    SC     538682       8811\n",
       "40      2021/03/15    SD     114649       1912\n",
       "41      2021/03/15    TN     785983      11627\n",
       "42      2021/03/15    TX    2721126      47807\n",
       "43      2021/03/15    UT     378600       2027\n",
       "44      2021/03/15    VA     597141      10104\n",
       "45      2021/03/15    VT      15761        213\n",
       "46      2021/03/15    WA     350342       5135\n",
       "47      2021/03/15    WI     625690       7177\n",
       "48      2021/03/15    WV     135678       2531\n",
       "49      2021/03/15    WY      55327        691"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_covid_data(month: int, day: int, year: int):\n",
    "    covid_data_url = r\".\\Cleaned Data\\state_covid_data.csv\"\n",
    "    covid_data = pd.read_csv(covid_data_url)\n",
    "    \n",
    "    date_filter = formatDate(month, day, year)\n",
    "    covid_data = covid_data[covid_data[\"submission_date\"] == date_filter]\n",
    "    covid_data.sort_values(\"state\", inplace=True)\n",
    "    covid_data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return covid_data\n",
    "\n",
    "def formatPreZero(num: int) -> str:\n",
    "    if num >= 10:\n",
    "        return str(num)\n",
    "    \n",
    "    return \"0\" + str(num)\n",
    "    \n",
    "    \n",
    "def formatDate(month: int,  day: int, year: int) -> str:\n",
    "    return f\"{year}/{formatPreZero(month)}/{formatPreZero(day)}\"\n",
    "\n",
    "read_covid_data(3, 15, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2 Code (Labor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copied from https://gist.github.com/rogerallen/1583593\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "\n",
    "def to_state_code(state_name):\n",
    "    return us_state_to_abbrev[state_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State    Total Blue_col White_col\n",
      "0     AL   2012.5    924.9     968.0\n",
      "1     AK    304.1    115.8     163.2\n",
      "2     AZ   2860.3   1191.2    1523.6\n",
      "3     AR   1257.5    576.4     599.2\n",
      "4     CA  15937.9   6396.3    8559.3\n",
      "5     CO   2626.7   1046.8    1380.1\n",
      "6     CT   1570.8    616.0     867.5\n",
      "7     DE    442.3    173.2     249.7\n",
      "8     FL     8492   3684.1    4353.2\n",
      "9     GA   4449.8   1962.2    2217.2\n",
      "10    HI    541.7    223.1     289.8\n",
      "11    ID    776.4    364.4     375.9\n",
      "12    IL   5637.3   2353.1    2960.5\n",
      "13    IN   3024.8   1545.6    1337.9\n",
      "14    IA   1513.4    722.0     715.3\n",
      "15    KS   1354.7    594.9     689.1\n",
      "16    KY   1841.8    883.8     869.3\n",
      "17    LA   1834.6    807.2     915.8\n",
      "18    ME    601.2    250.7     321.0\n",
      "19    MD   2603.9    949.4    1520.9\n",
      "20    MA   3356.3   1196.6    1959.0\n",
      "21    MI   4027.9   1793.7    2034.5\n",
      "22    MN   2719.6   1092.8    1483.4\n",
      "23    MS   1125.9    543.7     526.9\n",
      "24    MO   2799.2   1191.1    1448.7\n",
      "25    MT    476.4    207.5     238.4\n",
      "26    NE    998.3    430.3     515.5\n",
      "27    NV   1277.5    665.7     547.2\n",
      "28    NH    639.3    290.4     314.7\n",
      "29    NJ   3864.9   1527.5    2126.2\n",
      "30    NM    778.4    277.6     449.4\n",
      "31    NY   8723.3   2762.5    5358.0\n",
      "32    NC   4435.8   1983.5    2215.7\n",
      "33    ND    407.9    175.0     199.6\n",
      "34    OH   5277.8   2378.5    2636.1\n",
      "35    OK   1614.1    670.7     835.2\n",
      "36    OR   1798.2    787.4     914.5\n",
      "37    PA   5602.4   2298.6    2975.9\n",
      "38    RI    457.8    174.0     259.4\n",
      "39    SC   2117.3   1002.9    1011.2\n",
      "40    SD    431.6    195.7     212.8\n",
      "41    TN   3030.5   1407.9    1456.1\n",
      "42    TX  12365.2   5313.2    6278.9\n",
      "43    UT   1565.1    698.4     780.0\n",
      "44    VT    286.1    118.2     154.2\n",
      "45    VA   3881.1   1428.3    2201.8\n",
      "46    WA   3266.8   1358.6    1634.7\n",
      "47    WV    671.9    257.6     368.0\n",
      "48    WI   2831.1   1343.6    1293.9\n",
      "49    WY    273.7    116.5     124.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp/ipykernel_23784/1544146922.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  raw_labor_data[\"State\"] = raw_labor_data[\"State\"].str.replace('\\d+', '')\n",
      "C:\\Users\\ethan\\AppData\\Local\\Temp/ipykernel_23784/1544146922.py:21: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  raw_labor_data[\"State\"] = raw_labor_data[\"State\"].str.replace('(', '')\n",
      "C:\\Users\\ethan\\AppData\\Local\\Temp/ipykernel_23784/1544146922.py:22: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  raw_labor_data[\"State\"] = raw_labor_data[\"State\"].str.replace(')', '')\n"
     ]
    }
   ],
   "source": [
    "def clean_labor_data():\n",
    "    #Read excel file, renames first column to States and take out null rows\n",
    "    raw_labor_data = pd.read_excel(\"./Raw Data/collar_dataset_raw.xlsx\", header = 4)\n",
    "    raw_labor_data.rename(columns={\"Unnamed: 0\": \"State\"}, inplace=True)\n",
    "    raw_labor_data = raw_labor_data.dropna()\n",
    "\n",
    "    #Take out data from 2021 and only keep 2020\n",
    "    raw_labor_data = raw_labor_data[[\"State\", \"Dec.\\n2020\", \"Dec.\\n2020.1\", \"Dec.\\n2020.2\"]]\n",
    "    \n",
    "    non_states = [\"Virgin Islands\", \"District of Columbia\", \"Puerto Rico\"]\n",
    "\n",
    "    #Removes all non official states from dataset\n",
    "    for region in non_states:\n",
    "        raw_labor_data = raw_labor_data[raw_labor_data[\"State\"].str.contains(region)==False]\n",
    "\n",
    "    #Reset index to start at 0\n",
    "    raw_labor_data = raw_labor_data.reset_index(drop = True)\n",
    "\n",
    "    #eliminated extra characters in state names\n",
    "    raw_labor_data[\"State\"] = raw_labor_data[\"State\"].str.replace('\\d+', '')\n",
    "    raw_labor_data[\"State\"] = raw_labor_data[\"State\"].str.replace('(', '')\n",
    "    raw_labor_data[\"State\"] = raw_labor_data[\"State\"].str.replace(')', '')\n",
    "\n",
    "    #Convert state names into codes (First two letters of each state name)\n",
    "    raw_labor_data[\"State\"] = raw_labor_data[\"State\"].apply(lambda state_name: us_state_to_abbrev[state_name])\n",
    "\n",
    "    #Original raw data has different columns stacked on top of each row, so we need to reorder the dataset.\n",
    "    #Block 1 contains total, constructing and mining data\n",
    "    block1 = raw_labor_data[:50]\n",
    "    block1.columns = [\"State\", \"Total\", \"Constructing\", \"Mining\"]\n",
    "\n",
    "    #Block 2 contains Trade, Financial and Professional\n",
    "    block2 = raw_labor_data[50:100]\n",
    "    block2.columns = [\"State\", \"Trade\", \"Financial\", \"Professional\"]\n",
    "\n",
    "    #Block 3 contains Education, Leisure and Government\n",
    "    block3 = raw_labor_data[100:]\n",
    "    block3.columns = [\"State\", \"Education\", \"Leisure\", \"Gov\"]\n",
    "\n",
    "    #merge all blocks into one dataframe\n",
    "    labor_data = block1.merge(block2, on=\"State\")\n",
    "    labor_data = labor_data.merge(block3, on=\"State\")\n",
    "\n",
    "    #We only need data on white collar and blue collar, so we can combine each job sector to their respective group.\n",
    "    labor_data[\"Blue_col\"] = labor_data[\"Constructing\"] + labor_data[\"Mining\"] + labor_data[\"Trade\"] + labor_data[\"Leisure\"]\n",
    "    labor_data[\"White_col\"] = labor_data[\"Financial\"] + labor_data[\"Professional\"] + labor_data[\"Education\"] + labor_data[\"Gov\"]\n",
    "\n",
    "    #Get rid of all other columns except State, White_col, Blue_col and Total\n",
    "    labor_data.drop(columns = [\"Constructing\", \"Mining\", \"Trade\", \"Financial\", \"Professional\", \"Education\", \"Leisure\", \"Gov\"], inplace=True)\n",
    "    #export as csv\n",
    "    labor_data.to_csv('./Cleaned Data/state_labor_data.csv')\n",
    "    print(labor_data)\n",
    "\n",
    "clean_labor_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3 Code (Income/Insurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State  Persons without Health Insurance, % Median Household Income in 2020\n",
      "0     AL                                 11.7                          54,393\n",
      "1     AK                                 13.9                          74,476\n",
      "2     AZ                                 13.6                          66,628\n",
      "3     AR                                 10.9                          50,540\n",
      "4     CA                                  8.9                          77,358\n",
      "5     CO                                  9.3                          82,611\n",
      "6     CT                                  7.0                          79,043\n",
      "7     DE                                  8.1                          69,132\n",
      "8     FL                                 16.3                          57,435\n",
      "9     GA                                 15.5                          58,952\n",
      "10    HI                                  5.0                          80,729\n",
      "11    ID                                 12.8                          66,499\n",
      "12    IL                                  8.6                          73,753\n",
      "13    IN                                 10.3                          66,360\n",
      "14    IA                                  6.0                          68,469\n",
      "15    KS                                 10.9                          72,815\n",
      "16    KY                                  7.7                          56,525\n",
      "17    LA                                 10.5                          50,935\n",
      "18    ME                                 10.1                          63,440\n",
      "19    MD                                  6.9                          94,384\n",
      "20    MA                                  3.5                          86,725\n",
      "21    MI                                  6.9                          63,829\n",
      "22    MN                                  5.8                          78,461\n",
      "23    MS                                 15.4                          44,966\n",
      "24    MO                                 12.0                          61,901\n",
      "25    MT                                 10.2                          56,442\n",
      "26    NE                                  9.8                          72,024\n",
      "27    NV                                 13.4                          60,956\n",
      "28    NH                                  7.6                          88,235\n",
      "29    NJ                                  9.2                          85,239\n",
      "30    NM                                 12.0                          50,822\n",
      "31    NY                                  6.1                          68,304\n",
      "32    NC                                 13.4                          60,266\n",
      "33    ND                                  8.1                          63,657\n",
      "34    OH                                  7.8                          60,110\n",
      "35    OK                                 16.8                          52,341\n",
      "36    OR                                  8.6                          76,554\n",
      "37    PA                                  7.0                          70,117\n",
      "38    RI                                  4.8                          80,012\n",
      "39    SC                                 13.2                          60,097\n",
      "40    SD                                 12.2                          69,787\n",
      "41    TN                                 12.1                          54,665\n",
      "42    TX                                 20.8                          68,093\n",
      "43    UT                                 10.8                          83,670\n",
      "44    VT                                  5.6                          66,902\n",
      "45    VA                                  9.3                          81,947\n",
      "46    WA                                  7.7                          81,083\n",
      "47    WV                                  8.3                          51,615\n",
      "48    WI                                  6.8                          67,094\n",
      "49    WY                                 14.8                          65,108\n"
     ]
    }
   ],
   "source": [
    "# Cleaning socioeconomic data\n",
    "\n",
    "socioeconomic_data_url = r'./Raw Data/socioeconomic_data.csv'\n",
    "socioeconomic_data = pd.read_csv(socioeconomic_data_url)\n",
    "print(socioeconomic_data)\n",
    "\n",
    "# Saving to CSV\n",
    "clean_socioeconomic_data_url = r\"./Cleaned Data/clean_socioeconomic_data.csv\"\n",
    "socioeconomic_data.to_csv(clean_socioeconomic_data_url, index=False)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
